{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp3: Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于训练好的词向量，编写RNN模型用于文本分类：分析文本情感是正面或者是负面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T08:05:40.638757Z",
     "start_time": "2021-12-01T08:05:39.908710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0 cpu\n",
      "10.2\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"TRUE\"\n",
    "os.environ['CUDA_ENABLE_DEVICES'] = '0'\n",
    "\n",
    "# torch.cuda.set_device(0)\n",
    "device = torch.device('cpu' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.__version__, device)\n",
    "print(torch.version.cuda) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T08:05:40.654219Z",
     "start_time": "2021-12-01T08:05:40.639756Z"
    }
   },
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "seed = 20211129\n",
    "setup_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 微博评论数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T08:05:40.908705Z",
     "start_time": "2021-12-01T08:05:40.655216Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"weibo_senti_100k.csv\"\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 去除重复评论&停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T08:05:41.043345Z",
     "start_time": "2021-12-01T08:05:40.909537Z"
    }
   },
   "outputs": [],
   "source": [
    "#  (它们的 label 可能不一样)\n",
    "df = df.drop_duplicates(subset=['label', 'review'], keep='first')\n",
    "df = df.drop_duplicates(subset=['review'], keep=False).reset_index(drop=True)\n",
    "df = df.sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T08:06:30.197882Z",
     "start_time": "2021-12-01T08:05:41.044186Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\ustc\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.421 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 去除停用词\n",
    "def stopwords_list():\n",
    "    with open('哈工大停用词表.txt',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        result = [i.strip('\\n') for i in lines]\n",
    "    return result\n",
    "stopwords = stopwords_list()\n",
    "stopwords.append('...')\n",
    "\n",
    "review_list = []\n",
    "for i in df['review'].map(lambda x: ' '.join(jieba.cut(x))):\n",
    "    final=''\n",
    "    for j in i.split():\n",
    "        if j not in stopwords:\n",
    "            final +=\" \"+str(j)\n",
    "    review_list.append(final)\n",
    "df['review'] = review_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T08:06:30.227828Z",
     "start_time": "2021-12-01T08:06:30.198876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>﻿ 更博 爆照 帅 越来越 爱 生快 傻 缺 爱 爱 爱</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>张晓鹏 jonathan 土耳其 事要 认真对待 直接 开除 丁丁 看 世界 很 细心 酒...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>姑娘 都 羡慕 … 招财猫 高兴 … … 爱 蔓延 JC 小 学徒 一枚 明天 见 李欣芸...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>美 ~ ~ ~ ~ ~ 爱</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>梦想 大 舞台 大 ! 鼓掌</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             review\n",
       "0      1                       ﻿ 更博 爆照 帅 越来越 爱 生快 傻 缺 爱 爱 爱\n",
       "1      1   张晓鹏 jonathan 土耳其 事要 认真对待 直接 开除 丁丁 看 世界 很 细心 酒...\n",
       "2      1   姑娘 都 羡慕 … 招财猫 高兴 … … 爱 蔓延 JC 小 学徒 一枚 明天 见 李欣芸...\n",
       "3      1                                      美 ~ ~ ~ ~ ~ 爱\n",
       "4      1                                     梦想 大 舞台 大 ! 鼓掌"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 分词并创建词字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T08:06:30.242760Z",
     "start_time": "2021-12-01T08:06:30.228797Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_vocab(review_list):\n",
    "    word_set = set()\n",
    "    i = 0\n",
    "    for x in review_list:\n",
    "        word_set = word_set | set(x.split())\n",
    "        if i % 10000 == 0:\n",
    "            print(\"构建词集合 {}0k/116k\".format(i//10000), end='\\r')\n",
    "        i += 1\n",
    "    # 直接对每个词进行编号\n",
    "    vocab = dict(zip(word_set, range(1,len(word_set)+1)))\n",
    "    print('# words in vocab:', len(vocab))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T08:06:30.302599Z",
     "start_time": "2021-12-01T08:06:30.244766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# words in vocab: 200542\n"
     ]
    }
   ],
   "source": [
    "# random.shuffle(review_list)\n",
    "# vocab = get_vocab(review_list)\n",
    "\n",
    "import  pickle\n",
    "with open('vocab0_0.pickle', 'rb') as file:\n",
    "    vocab = pickle.load(file)\n",
    "print('# words in vocab:', len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 长度规整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T08:06:31.080524Z",
     "start_time": "2021-12-01T08:06:30.303598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "len_review = [len(review.split()) for review in df.review]\n",
    "plt.hist(len_review)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为每条评论长度不一致所以不能直接组合成小批量，定义preprocess函数对每条评论通过词典转换成词索引。由评论长度(词数)分布图，可以将所有评论通过截断或者补0，将每条评论长度固定成50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T08:06:31.095480Z",
     "start_time": "2021-12-01T08:06:31.082514Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(data, vocab):  \n",
    "    max_l = 50  # 将每条评论通过截断或者补0，使长度得到统一\n",
    "\n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))\n",
    "    \n",
    "    tokenized_data = [review.split() for review, _ in data]\n",
    "    features = torch.tensor([pad([vocab[word] for word in words]) for words in tokenized_data])\n",
    "    labels = torch.tensor([label for _, label in data])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 创建数据迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T08:06:32.307569Z",
     "start_time": "2021-12-01T08:06:31.096478Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = df.sample(frac=0.7, replace=False, random_state=seed)\n",
    "test_df = df.drop(index=train_df.index)\n",
    "train_data = train_df[['review', 'label']].values.tolist()\n",
    "test_data = test_df[['review', 'label']].values.tolist()\n",
    "random.shuffle(train_data)\n",
    "random.shuffle(test_data)\n",
    "# del df, train_df, test_df\n",
    "\n",
    "batch_size = 32\n",
    "train_data = Data.TensorDataset(*preprocess(train_data, vocab))\n",
    "test_data = Data.TensorDataset(*preprocess(test_data, vocab))   # 星号将返回的元组解开\n",
    "train_iter = Data.DataLoader(train_data, batch_size, shuffle=True)\n",
    "test_iter = Data.DataLoader(test_data, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印第一个小批量数据的形状以及训练集中小批量的个数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T08:06:32.322359Z",
     "start_time": "2021-12-01T08:06:32.308397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  5406,  20650,      0,  ...,      0,      0,      0],\n",
      "        [156070,  72011, 113260,  ...,  93679,  81067,  51217],\n",
      "        [183362,   6479,  86765,  ...,      0,      0,      0],\n",
      "        ...,\n",
      "        [  6479, 132962,  20401,  ...,      0,      0,      0],\n",
      "        [123254, 101384, 166360,  ...,      0,      0,      0],\n",
      "        [129617, 154672, 149896,  ...,      0,      0,      0]]) tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        1, 1, 0, 0, 1, 0, 1, 1])\n",
      "#batches: 2558\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_iter:\n",
    "#     print('X', X.shape, 'y', y.shape)\n",
    "    print(X, y)\n",
    "    break\n",
    "print('#batches:', len(train_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 使用RNN模型进行分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 网络搭建 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个模型中，每个词先通过嵌入层得到特征向量。然后，我们使用双向循环神经网络对特征序列进一步编码得到序列信息。最后，我们将编码的序列信息通过全连接层变换为输出。具体来说，我们可以将双向长短期记忆在最初时间步和最终时间步的隐藏状态连结，作为特征序列的表征传递给输出层分类。在下面实现的BiRNN类中，Embedding实例即嵌入层，LSTM实例即为序列编码的隐藏层，Linear实例即生成分类结果的输出层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T08:06:32.337320Z",
     "start_time": "2021-12-01T08:06:32.323357Z"
    }
   },
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab, embed_size, num_hiddens, num_layers):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        \n",
    "        # bidirectional设为True即得到双向循环神经网络\n",
    "        self.encoder = nn.LSTM(input_size=embed_size, \n",
    "                                hidden_size=num_hiddens, \n",
    "                                num_layers=num_layers,\n",
    "                                bidirectional=True)\n",
    "        # 初始时间步和最终时间步的隐藏状态作为全连接层输入\n",
    "        self.decoder = nn.Linear(4*num_hiddens, 2) \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs的形状是(批量大小, 词数)，因为LSTM需要将序列长度(seq_len)作为第一维，所以将输入转置后\n",
    "        # 再提取词特征，输出形状为(词数, 批量大小, 词向量维度)\n",
    "        embeddings = self.embedding(inputs.permute(1, 0))\n",
    "        print(inputs.shape, end=';')\n",
    "        print(embeddings.shape, end=';')\n",
    "        # rnn.LSTM只传入输入embeddings，因此只返回最后一层的隐藏层在各时间步的隐藏状态。\n",
    "        # outputs形状是(词数, 批量大小, 2 * 隐藏单元个数)\n",
    "        outputs, _ = self.encoder(embeddings) # output, (h, c)\n",
    "        print(outputs.shape, end=';')\n",
    "        # 连结初始时间步和最终时间步的隐藏状态作为全连接层输入。它的形状为\n",
    "        # (批量大小, 4 * 隐藏单元个数)。\n",
    "        encoding = torch.cat((outputs[0], outputs[-1]), -1)\n",
    "        outs = self.decoder(encoding)\n",
    "        print(encoding.shape, end=';')\n",
    "        print(outs.shape, end='\\r')\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个含两个隐藏层的双向循环神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T08:06:32.456998Z",
     "start_time": "2021-12-01T08:06:32.338317Z"
    }
   },
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers = 100, 100, 2\n",
    "net = BiRNN(vocab, embed_size, num_hiddens, num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 训练并评价模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义验证函数与训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T08:08:14.081842Z",
     "start_time": "2021-12-01T08:08:14.070576Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net, device):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            net.eval() # 评估模式\n",
    "            acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()\n",
    "            net.train() # 改回训练模式\n",
    "            n += y.shape[0]\n",
    "    return acc_sum / n\n",
    "\n",
    "def train(train_iter, test_iter, net, loss, optimizer, device, num_epochs):\n",
    "    net = net.to(device)\n",
    "    print(\"training on \", device)\n",
    "    batch_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            print('batch:', batch_count+1)\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            print('y:',y.shape)\n",
    "            print(\"y_hat:\", y_hat.shape)\n",
    "            l = loss(y_hat, y)\n",
    "            break\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = evaluate_accuracy(test_iter, net, device)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T08:08:26.528270Z",
     "start_time": "2021-12-01T08:08:21.770435Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cpu\n",
      "batch: 1\n",
      "y: torch.Size([32]));torch.Size([50, 32, 100]);torch.Size([50, 32, 200]);torch.Size([32, 400]);torch.Size([32, 2])\n",
      "y_hat: torch.Size([32, 2])\n",
      "torch.Size([32, 50]);torch.Size([50, 32, 100]);torch.Size([50, 32, 200]);torch.Size([32, 400]);torch.Size([32, 2])torch.Size([32, 400]);torch.Size([32, 2])torch.Size([32, 400]);torch.Size([32, 2])torch.Size([32, 400]);torch.Size([32, 2])torch.Size([32, 400]);torch.Size([32, 2])torch.Size([32, 400]);torch.Size([32, 2])torch.Size([32, 2])torch.Size([32, 400]);torch.Size([32, 2])torch.Size([32, 400]);torch.Size([32, 2])torch.Size([32, 400]);torch.Size([32, 2])torch.Size([50, 32, 200]);torch.Size([32, 400]);torch.Size([32, 2])torch.Size([32, 400]);torch.Size([32, 2])torch.Size([32, 400]);torch.Size([32, 2])torch.Size([32, 400]);torch.Size([32, 2])torch.Size([32, 400]);torch.Size([32, 2])\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_74348/1017151768.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_74348/2566004458.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_iter, test_iter, net, loss, optimizer, device, num_epochs)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mn\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mbatch_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n\u001b[0;32m     34\u001b[0m               % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_74348/2566004458.py\u001b[0m in \u001b[0;36mevaluate_accuracy\u001b[1;34m(data_iter, net, device)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0macc_sum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m             \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 评估模式\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0macc_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_profile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pytorch_gpu\\lib\\site-packages\\torch\\autograd\\profiler.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    430\u001b[0m         \u001b[1;31m# Stores underlying RecordFunction as a tensor. TODO: move to custom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m         \u001b[1;31m# class (https://github.com/pytorch/pytorch/issues/35026).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr, num_epochs = 0.001, 5\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train(train_iter, test_iter, net, loss, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在做数据清洗之前试着跑了几次，测试集准确率基本都在0.93-0.94；  \n",
    "由此可见以上的数据清洗工作还是很有成效的。  \n",
    "\n",
    "下面看看使用预训练的词向量，效果是否能有提升。  \n",
    "(估计使用预训练的词向量反而没那么好，因为这本身就是一个很简单的分类任务，第一代的错误率就只有 1% 了)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 改进方案 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 使用预训练的词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从 https://github.com/Embedding/Chinese-Word-Vectors 选取微博词库(Word)，每个词对应一个预训练好的300维向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T08:07:07.948970Z",
     "start_time": "2021-12-01T08:07:07.948970Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_pretrained_vocab(words):\n",
    "    vocab = {}\n",
    "    with open(\"sgns.weibo.word\", \"r\", encoding='utf-8') as f:\n",
    "        line = f.readline()     # 跳过第一行\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            strlist = line[:-2].split(' ')\n",
    "            if strlist[0] in words:\n",
    "                vocab[strlist[0]] = [float(s) for s in strlist[1:]]\n",
    "            line = f.readline()\n",
    "    print(\"vacab has been built successfully!\")\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T08:07:07.950009Z",
     "start_time": "2021-12-01T08:07:07.950009Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_pretrained_embedding(words, pretrained_vocab):\n",
    "    \"\"\"从预训练好的vocab中提取出words对应的词向量\"\"\"\n",
    "    embed = torch.zeros(len(words), len(list(glove_vocab.values())[0])) # 初始化为0\n",
    "    oov_count = 0 # out of vocabulary\n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "#             idx = pretrained_vocab.stoi[word]\n",
    "            embed[i, :] = torch.Tensor(pretrained_vocab[word])\n",
    "        except KeyError:\n",
    "            oov_count += 1\n",
    "    if oov_count > 0:\n",
    "        print(\"There are %d oov words.\" % oov_count)\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T08:07:07.950961Z",
     "start_time": "2021-12-01T08:07:07.950961Z"
    }
   },
   "outputs": [],
   "source": [
    "glove_vocab = load_pretrained_vocab(set(vocab.keys()))\n",
    "\n",
    "embed_size, num_hiddens, num_layers = 300, 100, 2\n",
    "net = BiRNN(vocab, embed_size, num_hiddens, num_layers)\n",
    "net.embedding.weight.data.copy_(load_pretrained_embedding(vocab, glove_vocab))\n",
    "net.embedding.weight.requires_grad = False # 直接加载预训练好的, 所以不需要更新它\n",
    "\n",
    "lr, num_epochs = 0.01, 5\n",
    "# 要过滤掉不计算梯度的embedding参数\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train(train_iter, test_iter, net, loss, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "竟然略有改善，但是区别不大。  \n",
    "\n",
    "下面对原始数据多做一些清洗工作，看看结果如何。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 数据深度清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T08:07:07.950961Z",
     "start_time": "2021-12-01T08:07:07.950961Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"weibo_senti_100k.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df = df.drop_duplicates(subset=['label', 'review'], keep='first')\n",
    "df = df.drop_duplicates(subset=['review'], keep=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了去除停用词（本身也包括一些特殊符号），再去掉网址，并对一些标点做合并。  \n",
    "比如将“美\\~\\~\\~\\~”变为“美\\~”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T08:07:07.951960Z",
     "start_time": "2021-12-01T08:07:07.951960Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def data_clean(x):\n",
    "    re_tag = r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b'  # HTML标签\n",
    "    # r'[a-z]*[:.]+\\S+'\n",
    "    x = re.sub(re_tag, '', x, flags=re.MULTILINE)\n",
    "    p_text = re.compile(u'[\\u4E00-\\u9FA5|\\s\\w]').findall(x)\n",
    "    x = re.sub(r'[\\d+___|_]', '', \"\".join(p_text))  # 去除特殊符号\n",
    "    x = re.sub(\",+\", \",\", x)  # 合并逗号\n",
    "    x = re.sub(\"~+\", \"~\", x)  # 合并 ~\n",
    "    x = re.sub(\" +\", \" \", x)  # 合并空格\n",
    "    x = re.sub(\"[...|…|。。。]+\", \"...\", x)  # 合并句号\n",
    "    x = re.sub(\"-+\", \"--\", x)  # 合并-\n",
    "    x = re.sub(\"———+\", \"———\", x)  # 合并-\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T08:07:07.952956Z",
     "start_time": "2021-12-01T08:07:07.952956Z"
    }
   },
   "outputs": [],
   "source": [
    "# 去除停用词\n",
    "def stopwords_list():\n",
    "    with open('哈工大停用词表.txt',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        result = [i.strip('\\n') for i in lines]\n",
    "    return result\n",
    "stopwords = stopwords_list()\n",
    "stopwords.append('...')\n",
    "\n",
    "review_list = []\n",
    "for i in df['review']:\n",
    "    i = data_clean(i)\n",
    "    i = ' '.join(jieba.cut(i))\n",
    "    final=''\n",
    "    for j in i.split():\n",
    "        if j not in stopwords:\n",
    "            final +=\" \"+str(j)\n",
    "    review_list.append(final)\n",
    "df['review'] = review_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T08:07:07.952956Z",
     "start_time": "2021-12-01T08:07:07.952956Z"
    }
   },
   "outputs": [],
   "source": [
    "random.shuffle(review_list)\n",
    "vocab = get_vocab(review_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T08:07:07.953953Z",
     "start_time": "2021-12-01T08:07:07.953953Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = df.sample(frac=0.7, replace=False, random_state=seed)\n",
    "test_df = df.drop(index=train_df.index)\n",
    "train_data = train_df[['review', 'label']].values.tolist()\n",
    "test_data = test_df[['review', 'label']].values.tolist()\n",
    "# del df, train_df, test_df\n",
    "\n",
    "batch_size = 64\n",
    "train_data = Data.TensorDataset(*preprocess(train_data, vocab))\n",
    "test_data = Data.TensorDataset(*preprocess(test_data, vocab))   # 星号将返回的元组解开\n",
    "train_iter = Data.DataLoader(train_data, batch_size, shuffle=True)\n",
    "test_iter = Data.DataLoader(test_data, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T08:07:07.954951Z",
     "start_time": "2021-12-01T08:07:07.954951Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for X, y in train_data:\n",
    "    print('X', X.shape, 'y', y.shape)\n",
    "    print(X, y)\n",
    "    break\n",
    "print('#batches:', len(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T08:07:07.954951Z",
     "start_time": "2021-12-01T08:07:07.954951Z"
    }
   },
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers = 50, 100, 2\n",
    "net = BiRNN(vocab, embed_size, num_hiddens, num_layers)\n",
    "# net.embedding.weight.data.copy_(load_pretrained_embedding(vocab, glove_vocab))\n",
    "# net.embedding.weight.requires_grad = False # 直接加载预训练好的, 所以不需要更新它\n",
    "\n",
    "lr, num_epochs = 0.001, 5\n",
    "# 要过滤掉不计算梯度的embedding参数\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "train(train_iter, test_iter, net, loss, optimizer, device, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-28T09:07:41.047959Z",
     "start_time": "2021-11-28T09:07:41.047959Z"
    }
   },
   "source": [
    "我花了一下午都没解决这个错误，google，stackoverflow，github都找了，很多说版本问题，我重新卸载并安装了最新版的pytorch和cudatoolkit还是没解决，所以实在是没找到具体原因，心态有点崩，只好暂时放弃。  \n",
    "本来还有一个想法的，暂时没有机会实现了，先记录一下：  \n",
    "把所有的表情找出来，同时记录它对应的label，分别有多少次是出现在正面评论，多少次是出现在负面评论。  \n",
    "虽然不知道具体应该如何指导网络的训练，但是直观上觉得这应该是情感分类中一个很重要的判断依据。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_gpu)",
   "language": "python",
   "name": "pytorch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "236.179px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 523,
   "position": {
    "height": "40px",
    "left": "268.997px",
    "right": "20px",
    "top": "118px",
    "width": "727.983px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
