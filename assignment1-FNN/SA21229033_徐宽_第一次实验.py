#!/usr/bin/env python
# coding: utf-8

# ## Exp1ï¼šFeed Forward Neural Network
# ä½¿ç”¨pytorchæˆ–è€…tensorflowå†™ä¸€ä¸ªå‰é¦ˆç¥ç»ç½‘ç»œï¼Œç”¨äºè¿‘ä¼¼æ­£å¼¦å‡½æ•°y=sin(x)ï¼Œğ‘¥âˆˆ[0,4ğœ‹)ï¼Œç ”ç©¶ç½‘ç»œæ·±åº¦ã€å­¦ä¹ ç‡ã€ç½‘ç»œå®½åº¦ã€æ¿€æ´»å‡½æ•°å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“

# SA21229033_å¾å®½_ç¬¬ä¸€æ¬¡å®éªŒ 

# ### åŸºæœ¬å®ç°è¿‡ç¨‹

# In[1]:


import torch
from torch import nn
import numpy as np
import time
from IPython import display
from matplotlib import pyplot as plt
import torch.utils.data as Data

get_ipython().run_line_magic('matplotlib', 'inline')
get_ipython().run_line_magic('config', "InlineBackend.figure_format = 'svg'")

# set random seed
def setup_seed(seed):
     torch.manual_seed(seed)
#      torch.cuda.manual_seed_all(seed)
     np.random.seed(seed)
#      random.seed(seed)
#      torch.backends.cudnn.deterministic = True


# In[2]:


# Data preparation
num_inputs = 1
num_examples = 10000
rate_test = 0.3
x_features = torch.tensor(np.random.rand(num_examples, num_inputs)*4*torch.pi, dtype=torch.float)
y_labels = torch.sin(x_features)
# y_labels += torch.tensor(np.random.normal(0, 0.01, size=y_labels.size()), dtype=torch.float)
# Train_set
trainfeatures = x_features[round(num_examples*rate_test):]
trainlabels = y_labels[round(num_examples*rate_test):]
print(trainfeatures.shape)
# Test_set
testfeatures = x_features[:round(num_examples*rate_test)]
testlabels = y_labels[:round(num_examples*rate_test)]
print(testfeatures.shape)


# In[3]:


# è¯»å–æ•°æ®
batch_size = 100
# å°†è®­ç»ƒæ•°æ®çš„ç‰¹å¾å’Œæ ‡ç­¾ç»„åˆ
dataset = Data.TensorDataset(trainfeatures, trainlabels)
# æŠŠ dataset æ”¾å…¥ DataLoader
train_iter = Data.DataLoader(
    dataset=dataset,  # torch TensorDataset format
    batch_size=batch_size,  # mini batch size
    shuffle=True,  # æ˜¯å¦æ‰“ä¹±æ•°æ® (è®­ç»ƒé›†ä¸€èˆ¬éœ€è¦è¿›è¡Œæ‰“ä¹±)
    num_workers=0,  # å¤šçº¿ç¨‹æ¥è¯»æ•°æ®ï¼Œ æ³¨æ„åœ¨Windowsä¸‹éœ€è¦è®¾ç½®ä¸º0
)
# å°†æµ‹è¯•æ•°æ®çš„ç‰¹å¾å’Œæ ‡ç­¾ç»„åˆ
dataset = Data.TensorDataset(testfeatures, testlabels)
# æŠŠ dataset æ”¾å…¥ DataLoader
test_iter = Data.DataLoader(
    dataset=dataset,
    batch_size=batch_size,
    shuffle=True,  
    num_workers=0,  
)


# In[4]:


# Fully connected neural network
class XKnet(nn.Module):
    def __init__(self, input_size, hidden_size, hidden_layers, activate):
        super(XKnet, self).__init__()
        self.hidden_layers = hidden_layers
        self.activate_fcs = {
            'relu': nn.ReLU(),
            'prelu': nn.PReLU(),
            'elu': nn.ELU(),
            'tanh': nn.Tanh()
        }
        self.fc_list = nn.ModuleList()
        self.fc_list.append(nn.Linear(input_size, hidden_size))
        for i in range(self.hidden_layers):
            # self.fc_list.append(nn.ReLU())
            self.fc_list.append(self.activate_fcs.get(activate))
            self.fc_list.append(nn.Linear(hidden_size, hidden_size))
        # self.fc_list.append(nn.ReLU())
        self.fc_list.append(self.activate_fcs.get(activate))
        self.fc_list.append(nn.Linear(hidden_size, 1))

    def forward(self, x):
        for fc in self.fc_list:
            x = fc(x)
        return x


# In[5]:


setup_seed(20211030)

# Hyper-parameters
input_size = 1
hidden_size = 64
hidden_layers = 4
num_epochs = 30
batch_size = 100
learning_rate = 0.001
activate = 'relu'

# Instantiation the model
xknet = XKnet(input_size, hidden_size, hidden_layers, activate)

# Loss and optimizer
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(xknet.parameters(), lr=learning_rate)


# In[6]:


# Train the model
t = time.time()
train_loss, test_loss = [], []
for epoch in range(num_epochs):
    for X, y in train_iter:  # xå’Œyåˆ†åˆ«æ˜¯å°æ‰¹é‡æ ·æœ¬çš„ç‰¹å¾å’Œæ ‡ç­¾
        optimizer.zero_grad()
        y_hat = xknet(X)
        loss = criterion(y, y_hat)
        loss.backward()
        optimizer.step()
    train_loss.append(criterion(xknet(trainfeatures), trainlabels).item())
    test_loss.append(criterion(xknet(testfeatures), testlabels).item())
    if (epoch+1) % 5 == 0:
        print('Epoch [{}/{}], train_loss: {:.6f}, test_loss: {:.6f}'
              .format(epoch+1, num_epochs, train_loss[epoch], test_loss[epoch]))
print('run_time: ', time.time()-t, 's')


# In[7]:


# plot loss curve
x = range(num_epochs)
plt.plot(x, train_loss, label="train_loss", linewidth=1.5)
plt.plot(x, test_loss, label="test_loss", linewidth=1.5)
plt.plot(x, np.zeros(len(x)), 'red', linestyle='--', linewidth=1)
plt.xlabel("epoch")
plt.ylabel("loss")
plt.legend()
plt.show()


# In[8]:


# plot prediction results
x = x_features[:200]
y = xknet(x).detach().numpy().reshape(1,-1)[0].tolist()
plt.scatter(x, y)

x = np.linspace(0, 4*np.pi)
plt.plot(x,np.sin(x), 'red')
plt.show()


# ### ç½‘ç»œæ·±åº¦å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“

# In[9]:


setup_seed(20211030)

# Hyper-parameters
input_size = 1
hidden_size = 64
# hidden_layers = 4
num_epochs = 30
batch_size = 100
learning_rate = 0.001
activate = 'relu'

time_list = []
x = range(num_epochs)
for hidden_layers in [1,2,4,7,10]:
    # Instantiation the model
    xknet = XKnet(input_size, hidden_size, hidden_layers, activate)

    # Loss and optimizer
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(xknet.parameters(), lr=learning_rate)

    # Train the model
    t = time.time()
    train_loss, test_loss = [], []
    for epoch in range(num_epochs):
        for X, y in train_iter:  # xå’Œyåˆ†åˆ«æ˜¯å°æ‰¹é‡æ ·æœ¬çš„ç‰¹å¾å’Œæ ‡ç­¾
            optimizer.zero_grad()
            y_hat = xknet(X)
            loss = criterion(y, y_hat)
            loss.backward()
            optimizer.step()
        train_loss.append(criterion(xknet(trainfeatures), trainlabels).item())
        test_loss.append(criterion(xknet(testfeatures), testlabels).item())
        
    time_list.append(time.time()-t)
    plt.plot(x, test_loss, label=str(hidden_layers), linewidth=1.5)

plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()
plt.show()


# ç”±ä¸Šå›¾å¯ä»¥çœ‹å‡ºï¼š  
# * éšè—å±‚åªæœ‰ä¸€å±‚çš„æ—¶å€™ï¼Œlossè¾¾åˆ°0.1å·¦å³å°±å¾ˆéš¾ç»§ç»­ä¸‹é™äº†ï¼Œè¿™è¯´æ˜æ­¤æ—¶ç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›éå¸¸æœ‰é™ï¼Œè¿˜ä¸è¶³ä»¥è¾ƒä¸ºç²¾ç¡®åœ°é€¼è¿‘æ­£å¼¦å‡½æ•°ã€‚  
# * å½“éšè—å±‚æ•°é‡è¾¾åˆ°2å±‚æ—¶ï¼Œlossä¸‹é™å¾—æ…¢ä¸€äº›ï¼Œä½†æ˜¯åˆ°åæœŸå·²ç»å¾ˆæ¥è¿‘0äº†ï¼Œè¯´æ˜æ­¤æ—¶å·²ç»å…·å¤‡è¾ƒå¥½çš„é€¼è¿‘èƒ½åŠ›ã€‚  
# * å½“éšè—å±‚æ•°é‡è¾¾åˆ°4å±‚ï¼Œ7å±‚ç”šè‡³10å±‚æ—¶ï¼Œlossç»§ç»­é™ä½ï¼Œè¡¨ç¤ºç½‘ç»œçš„æ‹Ÿåˆèƒ½åŠ›ç»§ç»­å¢å¼ºï¼Œä½†æ˜¯å¢é€Ÿæ”¾ç¼“ã€‚  

# In[10]:


plt.scatter([1,2,4,7,10],time_list)
plt.plot([1,2,4,7,10],time_list)
plt.xlabel('hidden_layers')
plt.ylabel('run_time')
plt.show()


# ä»è¿è¡Œæ—¶é—´ä¸Šçœ‹ï¼Œç½‘ç»œæ·±åº¦çš„å¢åŠ å°†ä¼šå¸¦æ¥è¿è¡Œæ—¶é—´çš„åŒæ¯”ä¾‹å¢åŠ ï¼Œå®ƒä»¬æ˜¯è¿‘ä¹çº¿æ€§çš„ã€‚

# ### ç½‘ç»œå®½åº¦å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“

# In[11]:


setup_seed(20211030)

# Hyper-parameters
input_size = 1
# hidden_size = 64
hidden_layers = 4
num_epochs = 30
batch_size = 100
learning_rate = 0.001
activate = 'relu'

time_list = []
x = range(num_epochs)
for hidden_size in [16,32,64,128]:
    # Instantiation the model
    xknet = XKnet(input_size, hidden_size, hidden_layers, activate)

    # Loss and optimizer
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(xknet.parameters(), lr=learning_rate)

    # Train the model
    t = time.time()
    train_loss, test_loss = [], []
    for epoch in range(num_epochs):
        for X, y in train_iter:  # xå’Œyåˆ†åˆ«æ˜¯å°æ‰¹é‡æ ·æœ¬çš„ç‰¹å¾å’Œæ ‡ç­¾
            optimizer.zero_grad()
            y_hat = xknet(X)
            loss = criterion(y, y_hat)
            loss.backward()
            optimizer.step()
        train_loss.append(criterion(xknet(trainfeatures), trainlabels).item())
        test_loss.append(criterion(xknet(testfeatures), testlabels).item())
        
    time_list.append(time.time()-t)
    plt.plot(x, test_loss, label=str(hidden_size), linewidth=1.5)

plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()
plt.show()


# å¯ä»¥çœ‹å‡ºåœ¨è¿­ä»£å‰æœŸï¼Œç½‘ç»œå®½åº¦è¶Šå¤§ï¼Œæ¨¡å‹çš„ç²¾åº¦è¶Šé«˜ï¼Œä½†æ˜¯15ä»£ä¹‹åå°±åŸºæœ¬æ²¡å¤šå¤§åŒºåˆ«äº†ï¼Œè¯´æ˜ç½‘ç»œå®½åº¦å¯¹äºæ¨¡å‹çš„ç²¾åº¦è€Œè¨€å½±å“ä¸å¤§

# In[12]:


plt.scatter([16,32,64,128],time_list)
plt.plot([16,32,64,128],time_list)
plt.xlabel('hidden_size')
plt.ylabel('run_time')
plt.show()


# è€Œè¿è¡Œæ—¶é—´ä¹Ÿå‡ ä¹æ˜¯éšç½‘ç»œå®½åº¦çº¿æ€§å¢é•¿çš„

# ### å­¦ä¹ ç‡å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“

# In[13]:


setup_seed(20211030)

# Hyper-parameters
input_size = 1
hidden_size = 64
hidden_layers = 4
num_epochs = 30
batch_size = 100
# learning_rate = 0.001
activate = 'relu'

time_list = []
x = range(num_epochs)
for learning_rate in [0.001, 0.01, 0.05, 0.1]:
    # Instantiation the model
    xknet = XKnet(input_size, hidden_size, hidden_layers, activate)

    # Loss and optimizer
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(xknet.parameters(), lr=learning_rate)

    # Train the model
    t = time.time()
    train_loss, test_loss = [], []
    for epoch in range(num_epochs):
        for X, y in train_iter:  # xå’Œyåˆ†åˆ«æ˜¯å°æ‰¹é‡æ ·æœ¬çš„ç‰¹å¾å’Œæ ‡ç­¾
            optimizer.zero_grad()
            y_hat = xknet(X)
            loss = criterion(y, y_hat)
            loss.backward()
            optimizer.step()
        train_loss.append(criterion(xknet(trainfeatures), trainlabels).item())
        test_loss.append(criterion(xknet(testfeatures), testlabels).item())
        
    time_list.append(time.time()-t)
    plt.plot(x, test_loss, label=str(learning_rate), linewidth=1.5)

plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()
plt.show()


# æ ¹æ®ä¸Šå›¾æˆ‘ä»¬å¯ä»¥å‘ç°ï¼š  
# * å½“å­¦ä¹ ç‡å–ä¸º0.01æˆ–0.001çš„æ—¶å€™ï¼Œè®­ç»ƒéƒ½æ˜¯æ­£å¸¸çš„ï¼Œlossæ›²çº¿ä¸€å¼€å§‹ä¸‹é™è¾ƒå¿«ï¼Œæ…¢æ…¢è¾¾åˆ°ç¨³å®šï¼›  
# * å½“å­¦ä¹ ç‡å–ä¸º0.05æˆ–0.1çš„æ—¶å€™ï¼Œlossæ›²çº¿å¹¶ä¸ä¸‹é™ï¼Œè€Œæ˜¯åå¤éœ‡è¡ï¼Œè¯´æ˜æ­¤æ—¶å­¦ä¹ ç‡è¿‡å¤§ï¼Œç½‘ç»œä¸æ”¶æ•›ã€‚

# In[14]:


plt.scatter([0.001, 0.01, 0.05, 0.1],time_list)
plt.plot([0.001, 0.01, 0.05, 0.1],time_list)
plt.xlabel('hidden_layers')
plt.ylabel('run_time')
plt.show()


# çœ‹èµ·æ¥ï¼Œå­¦ä¹ ç‡è¶Šå¤§ï¼Œè¿è¡Œæ—¶é—´è¶ŠçŸ­ï¼Œæˆ–è®¸æ˜¯ç”±äºæ­¥å­è¿ˆçš„é•¿ï¼Œå°±ä¼˜åŒ–å¾—å¿«ï¼Œä½†æ˜¯å®é™…ä¸Šæ€»çš„æ¥çœ‹ï¼Œæå·®å¾ˆå°ï¼Œè¯´æ˜å­¦ä¹ ç‡å¯¹äºç¨‹åºçš„è¿è¡Œæ—¶é—´å¹¶æ²¡æœ‰å¤ªå¤§çš„å½±å“ã€‚

# ### æ¿€æ´»å‡½æ•°å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“  

# In[15]:


setup_seed(20211030)

# Hyper-parameters
input_size = 1
hidden_size = 64
hidden_layers = 2
num_epochs = 30
batch_size = 100
learning_rate = 0.001

time_list = []
x = range(num_epochs)
print('è¿è¡Œæ—¶é—´ï¼š')
for activate in ['relu', 'prelu', 'elu', 'tanh']:
    # Instantiation the model
    xknet = XKnet(input_size, hidden_size, hidden_layers, activate)

    # Loss and optimizer
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(xknet.parameters(), lr=learning_rate)

    # Train the model
    t = time.time()
    train_loss, test_loss = [], []
    for epoch in range(num_epochs):
        for X, y in train_iter:  # xå’Œyåˆ†åˆ«æ˜¯å°æ‰¹é‡æ ·æœ¬çš„ç‰¹å¾å’Œæ ‡ç­¾
            optimizer.zero_grad()
            y_hat = xknet(X)
            loss = criterion(y, y_hat)
            loss.backward()
            optimizer.step()
        train_loss.append(criterion(xknet(trainfeatures), trainlabels).item())
        test_loss.append(criterion(xknet(testfeatures), testlabels).item())
        
    print(activate+':', round(time.time()-t,2), 's')
    plt.plot(x, test_loss, label=activate, linewidth=1.5)

plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()
plt.show()


# ä¸éš¾çœ‹å‡ºï¼Œä¸åŒçš„æ¿€æ´»å‡½æ•°å¯¹äºè®¡ç®—é€Ÿåº¦å½±å“ä¸å¤§ï¼Œä½†æ˜¯å¯¹äºæ¨¡å‹è¿­ä»£è¿‡ç¨‹çš„lossæ›²çº¿æœ‰ä¸€å®šçš„å½±å“ï¼š  
# * è¡¨ç°æœ€ä½³çš„æ˜¯ ELU å‡½æ•°ï¼ŒReLU å’Œ PReLU æ¬¡ä¹‹ï¼ŒTanh æ˜æ˜¾è¦é€Šè‰²ä¸€äº›ã€‚
